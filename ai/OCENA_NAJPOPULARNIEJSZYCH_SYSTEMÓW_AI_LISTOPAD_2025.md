# OCENA NAJPOPULARNIEJSZYCH SYSTEMÓW AI - LISTOPAD 2025
_Created by Ftarkowski_

**Data raportu:** 17 listopada 2025  
**Źródła:** Vectara HHEM Leaderboard, badania OpenAI, Anthropic, Google, GitHub, niezależne benchmarki 2025

## Metodologia Oceny
Każdy system oceniono w skali 1–10 według czterech kryteriów:
- **Użyteczność** – praktyczna wartość dla użytkowników
- **Błędy/Halucynacje** – dokładność i wiarygodność (10 = najmniej błędów)
- **Popularność** – zasięg i liczba użytkowników
- **Poprawność** – jakość outputu i zgodność z oczekiwaniami

Średnia końcowa to uśrednienie powyższych kryteriów.

---

## CHATBOTY I ASYSTENCI AI
1. **ChatGPT (OpenAI)**  
   - Użyteczność: 9.5/10  
   - Błędy: 8.5/10 (1.5% halucynacji według badań 2025)  
   - Popularność: 10/10 (5.2 miliarda miesięcznych wizyt)  
   - Poprawność: 9/10 (88.8% w MMLU)  
   - Uwagi: Bezsprzeczny lider rynku. GPT-4o ma jeden z najniższych wskaźników halucynacji (1.5%), ale GPT-5-mini spada do 4.9%. Modele reasoning (o3, o4) mają **wyższy** wskaźnik halucynacji (33% w PersonQA). Świetny do większości zastosowań, ale wymaga weryfikacji faktów w krytycznych obszarach (prawo, medycyna).  
   - **Średnia: 9.25/10**

2. **Gemini (Google)**  
   - Użyteczność: 9/10  
   - Błędy: 9.5/10 (0.7% halucynacji dla Gemini-2.0-Flash — najlepszy wynik)  
   - Popularność: 8/10  
   - Poprawność: 9/10  
   - Uwagi: Gemini-2.0-Flash-001 ma najniższy wskaźnik halucynacji ze wszystkich modeli (0.7%)! Gemini 2.5 Pro osiąga 1 milion tokenów kontekstu. Doskonała integracja z ekosystemem Google. Według użytkowników (wrzesień 2025) najlepiej oceniany model.  
   - **Średnia: 8.875/10**

3. **Claude (Anthropic)**  
   - Użyteczność: 9/10  
   - Błędy: 7.5/10 (4.4% dla Sonnet 4, 10.1% dla Opus 4.1)  
   - Popularność: 7.5/10  
   - Poprawność: 8.5/10  
   - Uwagi: Claude ma wysoki wskaźnik odmów (do 70% na niektórych testach) — woli nie odpowiedzieć niż udzielić błędnej odpowiedzi. Paradoksalnie Claude Opus 4.1 (większy model) ma więcej halucynacji niż Sonnet 4. Świetny do pisania i zadań etycznych. Okno kontekstu: 200K–1M tokenów.  
   - **Średnia: 8.125/10**

4. **DeepSeek**  
   - Użyteczność: 8/10  
   - Błędy: 8/10 (DeepSeek-R1: 97% dokładności w MATH-500)  
   - Popularność: 8.5/10 (wzrost 195% w lutym 2025)  
   - Poprawność: 8.5/10  
   - Uwagi: Chiński model, który zaskoczył rynek. 671 miliardów parametrów. Dorównuje OpenAI o1 w złożonych zadaniach rozumowania. Doskonały stosunek jakości do ceny.  
   - **Średnia: 8.25/10**

5. **Perplexity**  
   - Użyteczność: 8.5/10  
   - Błędy: 8/10  
   - Popularność: 7/10  
   - Poprawność: 8/10  
   - Uwagi: Specjalizuje się w wyszukiwaniu z dostępem do Internetu w czasie rzeczywistym. Idealne do badań i aktualnych informacji. Łączy wyszukiwanie z wieloma LLM.  
   - **Średnia: 7.875/10**

6. **Grok (xAI)**  
   - Użyteczność: 7.5/10  
   - Błędy: 7/10  
   - Popularność: 7/10  
   - Poprawność: 7.5/10  
   - Uwagi: Integracja z X (Twitter). Grok 4 przez 2 miesiące prowadził rankingi. Dostęp do danych w czasie rzeczywistym z platformy X.  
   - **Średnia: 7.25/10**

---

## NARZĘDZIA PROGRAMISTYCZNE AI
7. **GitHub Copilot**  
   - Użyteczność: 9/10  
   - Błędy: 7.5/10 (kontrowersyjne — niektóre badania wskazują na „spadek jakości kodu”)  
   - Popularność: 9.5/10 (lider dla programistów)  
   - Poprawność: 8/10  
   - Uwagi: Według oficjalnych badań GitHub: +56% szans na przejście testów, +13.6% więcej linii bez błędów. ALE — niezależne badania wskazują na „niepokojące trendy w utrzymywalności”. Kod szybszy o 55%, ale jakość budzi kontrowersje. Nowy model embeddings: +37.6% jakości wyszukiwania, +110% akceptacji dla C#.  
   - **Średnia: 8.5/10**

8. **OpenAI Codex (GPT-5-Codex)**  
   - Użyteczność: 9.5/10  
   - Błędy: 9/10 (51.3% w refaktoryzacji vs 33.9% dla GPT-5)  
   - Popularność: 8/10  
   - Poprawność: 9/10  
   - Uwagi: Wydany we wrześniu 2025. Może pracować autonomicznie przez 7+ godzin przy złożonych zadaniach. 77.2% na SWE-bench Verified. Dostępny w GitHub Copilot od 13 listopada 2025. To specjalista w kodowaniu.  
   - **Średnia: 8.875/10**

9. **Cursor**  
   - Użyteczność: 9/10  
   - Błędy: 8/10  
   - Popularność: 8.5/10 (bardzo szybki wzrost w 2025)  
   - Poprawność: 8.5/10  
   - Uwagi: Oparty na VS Code. Funkcja Composer do edycji wielu plików. $20/miesiąc Pro, $40/miesiąc Business. Świetna integracja AI. Jeden z najszybciej rosnących edytorów AI w 2025.  
   - **Średnia: 8.5/10**

10. **Windsurf (Codeium)**  
    - Użyteczność: 8.5/10  
    - Błędy: 8/10  
    - Popularność: 7.5/10  
    - Poprawność: 8/10  
    - Uwagi: $15/miesiąc start, $30/miesiąc dla zespołów. Agent Cascade z pamięcią między sesjami. Dobra alternatywa dla Cursor w niższej cenie.  
    - **Średnia: 8/10**

11. **Replit**  
    - Użyteczność: 8/10  
    - Błędy: 7.5/10  
    - Popularność: 8/10  
    - Poprawność: 7.5/10  
    - Uwagi: IDE w chmurze. $25/miesiąc Pro. Replit Agent generuje pełne aplikacje od zera. Świetny do nauki i współpracy, ale mniej zaawansowany niż Cursor/Windsurf.  
    - **Średnia: 7.75/10**

---

## NARZĘDZIA DO PROTOTYPOWANIA
12. **Bolt.new (StackBlitz)**  
    - Użyteczność: 9/10  
    - Błędy: 7.5/10  
    - Popularność: 9/10 (0 → $40M ARR w 6 miesięcy!)  
    - Poprawność: 8/10  
    - Uwagi: Fenomenalny wzrost — $40M ARR w 6 miesięcy! WebContainer w przeglądarce. Idealne do szybkich prototypów aplikacji webowych.  
    - **Średnia: 8.375/10**

13. **Lovable**  
    - Użyteczność: 8.5/10  
    - Błędy: 7/10  
    - Popularność: 8.5/10 (0 → $17M ARR w 3 miesiące!)  
    - Poprawność: 7.5/10  
    - Uwagi: Jeszcze szybszy wzrost — $17M ARR w 3 miesiące! $25/miesiąc Pro. 5 wiadomości/dzień darmowo. Konkurent Bolt.new.  
    - **Średnia: 7.875/10**

14. **v0 (Vercel)**  
    - Użyteczność: 8.5/10  
    - Błędy: 8/10  
    - Popularność: 8/10  
    - Poprawność: 8.5/10  
    - Uwagi: Koncentruje się na komponentach React. Konwersja Figma → kod. Integracja z ekosystemem React/Next.js. Solidna jakość, lepsza dla komponentów niż pełnych aplikacji.  
    - **Średnia: 8.25/10**

---

## GENEROWANIE OBRAZÓW AI
15. **Midjourney**  
    - Użyteczność: 9/10  
    - Błędy: 8/10 (niski wskaźnik błędów, ale trudności z tekstem)  
    - Popularność: 10/10 (złoty standard generowania obrazów)  
    - Poprawność: 9.5/10 (najwyższa jakość artystyczna)  
    - Uwagi: V7 wydana 3 kwietnia 2025. „Złoty standard” jakości AI. Najlepszy w wpływie artystycznym — 72% preferowanych w testach. Doskonałe światło, kompozycja, styl. Słabość: tekst w obrazach, interfejs Discord. Rozdzielczość: 2048x2048 px. $200M rocznych przychodów.  
    - **Średnia: 9.125/10**

16. **DALL·E 3 (OpenAI)**  
    - Użyteczność: 9/10  
    - Błędy: 8.5/10  
    - Popularność: 9/10  
    - Poprawność: 9/10 (94% dokładności w interpretacji promptów!)  
    - Uwagi: Najlepsza dokładność promptów (94%)! Doskonała integracja z ChatGPT. Świetny do precyzyjnych wizualizacji produktów. Szybki i niezawodny. Lepszy niż Midjourney do komercyjnych zastosowań wymagających dokładności.  
    - **Średnia: 8.875/10**

17. **Stable Diffusion**  
    - Użyteczność: 8/10  
    - Błędy: 7.5/10 (zależy od modelu)  
    - Popularność: 8.5/10  
    - Poprawność: 8/10  
    - Uwagi: Najlepsza kontrola i customizacja. Open source. Idealne do spójności postaci, brandingu. Wymaga wiedzy technicznej. Różnorodność jakości zależna od wybranego modelu.  
    - **Średnia: 8/10**

18. **Canva AI**  
    - Użyteczność: 9.5/10  
    - Błędy: 7/10  
    - Popularność: 10/10 (996M wizyt miesięcznie — 2. miejsce ogólnie!)  
    - Poprawność: 7.5/10  
    - Uwagi: Masowa popularność (996M wizyt!). Świetna dla nie-designerów. Łatwość użycia. Jakość AI niższa niż Midjourney/DALL·E, ale kompensuje to ekosystemem narzędzi designerskich.  
    - **Średnia: 8.5/10**

---

## MODELE OPEN SOURCE
19. **LLaMA 4 (Meta)**  
    - Użyteczność: 8/10  
    - Błędy: 7.5/10 (4.6–4.7% halucynacji)  
    - Popularność: 9/10  
    - Poprawność: 8/10  
    - Uwagi: Najpotężniejszy otwarty model. LLaMA 4 Scout: 10M tokenów kontekstu (~7 500 stron). Maverick i Scout: 4.6–4.7% halucynacji. Doskonały do badań i zastosowań on-premise.  
    - **Średnia: 8.125/10**

20. **Mixtral (Mistral AI)**  
    - Użyteczność: 8/10  
    - Błędy: 8/10  
    - Popularność: 7/10  
    - Poprawność: 8/10  
    - Uwagi: Architektura mixture-of-experts. Mistral Medium 3: wydajność frontier za $0.40/M tokenów — rewelacyjny stosunek ceny do jakości! Mixtral-8x22B to solidny model open source.  
    - **Średnia: 7.75/10**

21. **Qwen (Alibaba)**  
    - Użyteczność: 7.5/10  
    - Błędy: 7/10  
    - Popularność: 6.5/10  
    - Poprawność: 7.5/10  
    - Uwagi: Qwen 3 4B: 4 miliardy parametrów, 100+ języków. Dobry model chiński, mniejsza obecność globalna.  
    - **Średnia: 7.125/10**

22. **GPT-OSS-120B (OpenAI)**  
    - Użyteczność: 7/10  
    - Błędy: 7.5/10  
    - Popularność: 6/10  
    - Poprawność: 7.5/10  
    - Uwagi: Pierwszy open-weight model OpenAI od GPT-2! Wydany w sierpniu 2025. 117B parametrów (5.1B aktywnych). Ważny krok dla otwartej nauki.  
    - **Średnia: 7/10**

---

## MODELE SPECJALISTYCZNE
23. **o3 / o4 (OpenAI) — Reasoning Models**  
    - Użyteczność: 8/10  
    - Błędy: 6/10 (wysoki wskaźnik halucynacji — 33% w PersonQA!)  
    - Popularność: 7/10  
    - Poprawność: 7/10  
    - Uwagi: Problem: o3/o4 mają wyższy wskaźnik halucynacji niż poprzednie modele! OpenAI przyznaje: „potrzebne więcej badań”. Lepsze w kodowaniu i matematyce, ale generują więcej błędnych twierdzeń. Potrafią „wymyślać”, że uruchamiały kod „poza ChatGPT”.  
    - **Średnia: 7/10**

24. **DeepSeek R1 — Reasoning**  
    - Użyteczność: 8.5/10  
    - Błędy: 9/10 (97% w MATH-500)  
    - Popularność: 7/10  
    - Poprawność: 9/10  
    - Uwagi: Lepszy od o3 w zadaniach rozumowania! 97% dokładności w MATH-500. Dorównuje o1. Świetna alternatywa dla modeli reasoning OpenAI bez ich problemów z halucynacjami.  
    - **Średnia: 8.375/10**

---

## PODSUMOWANIE – TOP 10 OGÓLNIE
- ChatGPT (9.25/10) — uniwersalny lider
- Midjourney (9.125/10) — król obrazów artystycznych
- Gemini (8.875/10) — najmniej błędów, świetna jakość
- DALL·E 3 (8.875/10) — najlepsza precyzja promptów
- OpenAI Codex (8.875/10) — specjalista od kodu
- GitHub Copilot (8.5/10) — najpopularniejszy dla devów
- Cursor (8.5/10) — najlepsze IDE z AI
- Canva AI (8.5/10) — masowa użyteczność
- DeepSeek R1 (8.375/10) — najlepszy reasoning
- Bolt.new (8.375/10) — fenomenalny wzrost

## NAJWAŻNIEJSZE WNIOSKI 2025
**Halucynacje (ranking od najlepszych):**
1. Gemini 2.0 Flash: 0.7% ⭐
2. Gemini 2.5 Pro: 0.8%
3. o3-mini: 0.8%
4. ChatGPT GPT-4o: 1.5%
5. Gemini 2.5 Flash: 2.9%
6. Claude Sonnet 4: 4.4%
7. Claude Opus 4.1: 4.2%
8. LLaMA 4: 4.6–4.7%
9. ChatGPT-5 mini: 4.9%
10. Claude Opus 4: 10.1%
11. o3/o4: 33% (PersonQA) ⚠️

**Trendy:**
- Okna kontekstu rosną do 1–10M tokenów
- Multimodalność (tekst + obraz + audio + wideo) to standard
- Modele reasoning (o3, DeepSeek R1) mają paradoksalnie więcej halucynacji
- Open source dogania proprietary (LLaMA 4, Mixtral)
- Narzędzia AI-dev eksplodowały (Bolt: $40M ARR w 6 miesięcy!)

**Największe niespodzianki:**
- Gemini ma najmniej halucynacji (0.7%)
- Modele o3/o4 mają więcej błędów niż GPT-4o
- Claude Opus 4 ma więcej błędów niż mniejszy Sonnet 4
- Bolt.new i Lovable: bezprecedensowy wzrost ($40M i $17M ARR)
- DeepSeek R1 dorównuje OpenAI o1 za ułamek ceny
